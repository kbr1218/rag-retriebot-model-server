{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Router Chain Test\n",
    "#### *LLM ì²´ì¸ ë¼ìš°íŒ… ì ìš©í•˜ê¸°*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 00. ê¸°ë³¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# API KEY ì •ë³´ë¡œë“œ\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv('API_KEY_GEMINI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "lgdx_team2_routerchain\n"
     ]
    }
   ],
   "source": [
    "# LangSmith ì¶”ì ì„ ì„¤ì •í•©ë‹ˆë‹¤. https://smith.langchain.com\n",
    "# !pip install langchain-teddynote\n",
    "from langchain_teddynote import logging\n",
    "\n",
    "# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.\n",
    "logging.langsmith(\"lgdx_team2_routerchain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "# ë­ì²´ì¸ í™˜ê²½ ì„¤ì •\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser\n",
    "\n",
    "# VectorDB - FAISS\n",
    "from langchain_community.vectorstores import FAISS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 01. ë²¡í„°DB ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_5196\\3142432257.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name='ibm-granite/granite-embedding-278m-multilingual')\n",
      "c:\\workspaces\\LGDXteam2\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### ì„ë² ë”© ëª¨ë¸ ìƒì„±\n",
    "# https://huggingface.co/ibm-granite/granite-embedding-278m-multilingual\n",
    "embeddings = HuggingFaceEmbeddings(model_name='ibm-granite/granite-embedding-278m-multilingual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ\n",
    "new_vector_store = FAISS.load_local(\"../movie_4000_vectorstore_faiss\",\n",
    "                                    embeddings=embeddings,\n",
    "                                    allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 02. Router Chain: ì‚¬ìš©ì ì§ˆë¬¸ ìœ í˜• êµ¬ë¶„\n",
    "\n",
    "- ì •ë³´ê²€ìƒ‰\n",
    "- ì¶”ì²œìš”ì²­\n",
    "- ì¼ë°˜ëŒ€í™” (`default_chain`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. ì‚¬ìš©ì ì…ë ¥ê°’ ìœ í˜• ë¶„ë¥˜ìš© í”„ë¡¬í”„íŠ¸\n",
    "classification_template = \"\"\"ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë‹¤ìŒ ì„¸ ê°€ì§€ ë²”ì£¼ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:\n",
    "1ï¸âƒ£ **\"ì •ë³´ê²€ìƒ‰\"**: íŠ¹ì • ì˜í™”, ë“œë¼ë§ˆ, ë°°ìš°, ê°ë…, ëŸ¬ë‹íƒ€ì„, ê°œë´‰ ì—°ë„, ìˆ˜ìƒ ë‚´ì—­, í•„ëª¨ê·¸ë˜í”¼ ë“± **ì‚¬ì‹¤ì ì¸ ì •ë³´ë¥¼ ì°¾ëŠ” ì§ˆë¬¸**\n",
    "   - ê¸°ëŒ€ë˜ëŠ” ì‘ë‹µ ì˜ˆì‹œ: ë°°ìš°ê°€ ì¶œì—°í•œ ë“œë¼ë§ˆ/ì˜í™” ëª©ë¡, íŠ¹ì • ì—°ë„ì˜ ê°œë´‰ì‘ ë¦¬ìŠ¤íŠ¸ ë“±\n",
    "2ï¸âƒ£ **\"ì¶”ì²œìš”ì²­\"**: íŠ¹ì • ì¥ë¥´, ë°°ìš°, í…Œë§ˆ(ì˜ˆ: ì¢€ë¹„, ì‹œê°„ì—¬í–‰), ê°ì„±(ì˜ˆ: íë§, ê¸´ì¥ê°) ë“±ì— ëŒ€í•œ **ì¶”ì²œì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸**\n",
    "   - ê¸°ëŒ€ë˜ëŠ” ì‘ë‹µ ì˜ˆì‹œ: íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì˜í™”/ë“œë¼ë§ˆ ì¶”ì²œ\n",
    "3ï¸âƒ£ **\"ì¼ë°˜ëŒ€í™”\"**: ì„œë¹„ìŠ¤ì™€ ë¬´ê´€í•œ ì¼ë°˜ì ì¸ ëŒ€í™” (ì˜ˆ: ë‚ ì”¨, AI ê´€ë ¨ ì§ˆë¬¸, ì¡ë‹´)\n",
    "\n",
    "#### **ì˜ˆì‹œ í˜•ì‹**\n",
    "{{\n",
    "  \"type\": \"ì •ë³´ê²€ìƒ‰\"\n",
    "}}\n",
    "\n",
    "<user_input>\n",
    "{user_input}\n",
    "</user_input>\n",
    "\"\"\"\n",
    "classification_prompt = ChatPromptTemplate.from_template(classification_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LLMì„ ì´ìš©í•œ ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì²´ì¸\n",
    "classification_chain = (\n",
    "  classification_prompt\n",
    "  | ChatGoogleGenerativeAI(model='gemini-1.5-flash', api_key=GEMINI_API_KEY)\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "  \"type\": \"ì •ë³´ê²€ìƒ‰\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"ì¶”ì²œìš”ì²­\"\n",
      "}\n",
      "```\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"type\": \"ì¼ë°˜ëŒ€í™”\"\n",
      "}\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ì§ˆë¬¸ ë¶„ë¥˜ í…ŒìŠ¤íŠ¸\n",
    "# ì •ë³´ ê²€ìƒ‰ ì˜ˆìƒ ì§ˆë¬¸\n",
    "print(classification_chain.invoke({'user_input': \"2023ë…„ì— ê°œë´‰í•œ ì•¡ì…˜ ì˜í™” ë­ ìˆì–´?\"}))\n",
    "\n",
    "# ì¶”ì²œ ìš”ì²­ ì˜ˆìƒ ì§ˆë¬¸\n",
    "print(classification_chain.invoke({'user_input': 'ë””ì¹´í”„ë¦¬ì˜¤ê°€ ì£¼ì—°í•œ ì˜í™” ì¶”ì²œí•´ì¤˜.'}))\n",
    "\n",
    "# ì¼ë°˜ ëŒ€í™” ì˜ˆìƒ ì§ˆë¬¸\n",
    "print(classification_chain.invoke({'user_input': \"ë„ˆê°€ ì œì¼ ì¢‹ì•„í•˜ëŠ” ì˜í™” ë­ì•¼?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥´ê²Œ íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜\n",
    "def preprocess_classification_result(user_input: str):\n",
    "    classification_result = classification_chain.invoke({\"user_input\": user_input})\n",
    "\n",
    "    # JSON ë¬¸ìì—´ì—ì„œ ```json``` ì œê±° (ì •ê·œì‹ í™œìš©)\n",
    "    json_match = re.search(r'```json\\s*(\\{.*?\\})\\s*```', classification_result, re.DOTALL)\n",
    "    if json_match:\n",
    "        clean_json_str = json_match.group(1)                       # ì¤‘ê´„í˜¸ {} ë‚´ë¶€ë§Œ ì¶”ì¶œ\n",
    "    else:\n",
    "        clean_json_str = classification_result                     # ```json``` íƒœê·¸ê°€ ì—†ì„ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "\n",
    "    # JSON ë¬¸ìì—´ì„ dict ìë£Œí˜•ìœ¼ë¡œ ë³€í™˜\n",
    "    try:\n",
    "        classification_data = json.loads(clean_json_str)\n",
    "        type_value = classification_data.get(\"type\", \"ì¼ë°˜ëŒ€í™”\")   # ê¸°ë³¸ê°’ì„ 'ì¼ë°˜ëŒ€í™”'ë¡œ ì„¤ì •\n",
    "        # keywords = classification_data.get(\"keywords\", [])         # ê¸°ë³¸ê°’ì„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"âš ï¸ JSONDecodeError: {e}\")  # JSON ë³€í™˜ ì˜¤ë¥˜ í™•ì¸\n",
    "        type_value = \"ì¼ë°˜ëŒ€í™”\"           # JSON ë³€í™˜ ì˜¤ë¥˜ ì‹œ ê¸°ë³¸ê°’ ì„¤ì •\n",
    "        # keywords = []\n",
    "    \n",
    "    # return {\"type\": type_value, \"keywords\": keywords}\n",
    "    return {\"type\": type_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë”•ì…”ë„ˆë¦¬ ìë£Œí˜•ì„ stringìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜\n",
    "def format_change(classification_result: dict) -> str:\n",
    "    type_value = classification_result.get(\"classification_result\", {}).get(\"type\", \"ì¼ë°˜ëŒ€í™”\")\n",
    "    # keywords = classification_result.get(\"classification_result\", {}).get(\"keywords\", [])\n",
    "    user_input = classification_result.get(\"user_input\", \"\")\n",
    "\n",
    "    # string ìë£Œí˜•ìœ¼ë¡œ ë³€ê²½\n",
    "    # formatted_str = f\"type: '{type_value}', keywords: {keywords}, user_input: '{user_input}'\"\n",
    "    formatted_str = f\"type: '{type_value}', user_input: '{user_input}'\"\n",
    "    return formatted_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 03. Destination Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *1. default-chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> model loaded...\n"
     ]
    }
   ],
   "source": [
    "# default_chain ìƒì„± (ì‚¬ìš©ìì˜ ì˜ë¯¸ì—†ëŠ” ì…ë ¥ê°’ì— ëŒ€í•´ ì •í•´ì§„ ë‹µë³€ì„ í•  ë•Œ)\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "default_template = \"\"\"\n",
    "\"You are a chatbot that must always respond with 'ğŸ¶: ë©ë©!'.\n",
    "No matter what question the user asks, always reply with 'ğŸ¶: ë©ë©!'\"\n",
    "\n",
    "[ì‚¬ìš©ì ì…ë ¥ê³¼ ë¶„ë¥˜ ê²°ê³¼]:\n",
    "{classification_result}\n",
    "\"\"\"\n",
    "default_prompt = ChatPromptTemplate.from_template(default_template)\n",
    "\n",
    "# Google Gemini ëª¨ë¸ ìƒì„±\n",
    "def load_gemini():\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model='gemini-1.5-flash',\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        api_key=GEMINI_API_KEY\n",
    "    )\n",
    "    print(\">>>>>>> model loaded...\")\n",
    "    return model\n",
    "\n",
    "default_llm = load_gemini()\n",
    "\n",
    "# langchain ì²´ì¸ êµ¬ì„±\n",
    "default_chain = (\n",
    "  {\"classification_result\": RunnablePassthrough()}\n",
    "  | default_prompt               # í•˜ë‚˜ë¡œ ë§Œë“  ë¬¸ì„œë¥¼ promptì— ë„˜ê²¨ì£¼ê³ \n",
    "  | default_llm            # llmì´ ì›í•˜ëŠ” ë‹µë³€ì„ ë§Œë“¦\n",
    "  # | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *2. search-chain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><hr>\n",
    "\n",
    "#### *3. recommendation-chain*\n",
    "- ì¶”ì²œ ëª©ë¡ì„ ë°˜í™˜í•˜ëŠ” ì²´ì¸\n",
    "- `next_input`ì´ í•„ìš” ì—†ìŒ + ì •í•´ì§„ ìë£Œí˜•ìœ¼ë¡œ ë‹µí•´ì•¼ í•¨  \n",
    "  => `RouterOutputParser`ë¥¼ ì‚¬ìš©í•´ë³´ëŠ” ê²Œ ì¢‹ì„ ê±° ê°™ìŒ..~\n",
    "- ì´ ì²´ì¸ ë’¤ì— ì‚¬ìš©ì ì‹œì²­ ê¸°ë¡ ê¸°ë°˜ìœ¼ë¡œ ë°˜í™˜ëœ ì¶”ì²œ ëª©ë¡ì—ì„œ 5ê°œë¥¼ ì •í•˜ëŠ” ì‘ì—…ì„ í•´ì•¼ í•¨\n",
    "\n",
    "<br>\n",
    "\n",
    "- ì¥ë¥´ ê¸°ë°˜ ì¶”ì²œ â‡’ ì¥ë¥´\n",
    "- ì¤„ê±°ë¦¬(í‚¤ì›Œë“œ/ì»¨ì…‰) ê¸°ë°˜ ì¶”ì²œ â‡’ ì¤„ê±°ë¦¬\n",
    "- íŠ¹ì • ì—°ë„ ë° ì‹œëŒ€ë³„ ì½˜í…ì¸  ì¶”ì²œ â‡’ ì¤„ê±°ë¦¬ì— ì–¸ê¸‰ë˜ëŠ” ì‹œëŒ€ë°°ê²½/ì—°ë„\n",
    "==================================================\n",
    "- ì½˜í…ì¸  ì •ë³´ ê¸°ë°˜ ì¶”ì²œ â‡’ ì–´ë–¤ í–‰ì´ ìˆëŠ”ì§€ ë´ì•¼í• ë“¯\n",
    "- ì¸ê¸° ìˆëŠ” ì½˜í…ì¸  ì¶”ì²œ â‡’ ì‹œì²­íšŸìˆ˜ê°€ ë§ì€ ê²ƒ\n",
    "- ë¦¬ë·°(ê°ì„±) ê¸°ë°˜ ì¶”ì²œ â‡’ ë¦¬ë·°\n",
    "- ì‚¬ìš©ì ì„ í˜¸ ê¸°ë°˜ ì¶”ì²œ â‡’ ì‚¬ìš©ì ì‹œì²­ê¸°ë¡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>> model loaded...\n"
     ]
    }
   ],
   "source": [
    "# ê²€ìƒ‰ê¸° ìƒì„±\n",
    "retriever = new_vector_store.as_retriever(\n",
    "    search_type=\"mmr\",   \n",
    "    search_kwargs={\"k\": 20,              # ë°˜í™˜í•  ë¬¸ì„œ ìˆ˜ (default: 4)\n",
    "                   \"fetch_k\": 50,       # MMR ì•Œê³ ë¦¬ì¦˜ì— ì „ë‹¬í•  ë¬¸ì„œ ìˆ˜\n",
    "                   \"lambda_mult\": 0.5,    # ê²°ê³¼ ë‹¤ì–‘ì„± ì¡°ì ˆ (default: 0.5),\n",
    "                   }\n",
    ")\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì •\n",
    "template = \"\"\"\n",
    "You are a movie-recommendation chatbot.\n",
    "You must only answer based on the given context.\n",
    "Do not generate answers that are not directly supported by the context.\n",
    "ì•„ë˜ JSON ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”.  \n",
    "JSON ì´ì™¸ì˜ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.  \n",
    "ì¶”ì²œ ìˆœì„œëŠ” `id`ë¡œ í‘œì‹œí•˜ë©°, `genre`(ì¥ë¥´)ì™€ `title`(ì˜í™” ì œëª©)ì€ retrieved_contextì—ì„œ ê°€ì ¸ì˜¨ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "---\n",
    "ì˜ˆì œ ì¶œë ¥ í˜•ì‹:\n",
    "```json\n",
    "{{\n",
    "    {{\"id\": 1, \"genre\": \"ë‹¤íë©˜í„°ë¦¬\", \"title\": \"ê²¬ìë‹¨ì˜ ìš©í˜¸ë¬´\"}},\n",
    "    {{\"id\": 2, \"genre\": \"ë“œë¼ë§ˆ\", \"title\": \"ìŠ¤í”„ë§ ì†¡\"}},\n",
    "    {{\"id\": 3, \"genre\": \"ë“œë¼ë§ˆ\", \"title\": \"ë””ì–´ ë§ˆì´ í”„ë Œë“œ\"}}\n",
    "}}\n",
    "\n",
    "[ì‚¬ìš©ì ì…ë ¥ê³¼ ì‚¬ìš©ì ì…ë ¥ê°’ì˜ ìœ í˜• ë° í‚¤ì›Œë“œ]:\n",
    "{user_input}\n",
    "\n",
    "[Context]:\n",
    "{retrieved_context}\n",
    "\n",
    "[Answer]:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# Google Gemini ëª¨ë¸ ìƒì„±\n",
    "def load_gemini(system_instruction):\n",
    "    model = ChatGoogleGenerativeAI(\n",
    "        model='gemini-1.5-flash',\n",
    "        temperature=0.3,\n",
    "        max_tokens=5000,\n",
    "        system_instruction=system_instruction,\n",
    "        api_key=GEMINI_API_KEY\n",
    "    )\n",
    "    print(\">>>>>>> model loaded...\")\n",
    "    return model\n",
    "\n",
    "system_instruction = \"\"\"you are a movie-recommendation chatbot. you must answer based on given data.\"\"\"\n",
    "llm = load_gemini(system_instruction)\n",
    "\n",
    "# langchain ì²´ì¸ êµ¬ì„±\n",
    "recommendation_chain = (\n",
    "  {\"user_input\":RunnablePassthrough(),\n",
    "    \"retrieved_context\": retriever,\n",
    "  }\n",
    "  # question(ì‚¬ìš©ìì˜ ì§ˆë¬¸) ê¸°ë°˜ìœ¼ë¡œ ì—°ê´€ì„±ì´ ë†’ì€ ë¬¸ì„œ retriever ìˆ˜í–‰ >> format_docsë¡œ ë¬¸ì„œë¥¼ í•˜ë‚˜ë¡œ ë§Œë“¦\n",
    "  | prompt               # í•˜ë‚˜ë¡œ ë§Œë“  ë¬¸ì„œë¥¼ promptì— ë„˜ê²¨ì£¼ê³ \n",
    "  | llm                  # llmì´ ì›í•˜ëŠ” ë‹µë³€ì„ ë§Œë“¦\n",
    "  | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><hr>\n",
    "\n",
    "## 04. Full Chain ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_input(classification_result: dict):\n",
    "    # ì‚¬ìš©ìì˜ ì…ë ¥ ìœ í˜• ë¶„ë¥˜\n",
    "    classification_data = classification_result.get(\"classification_result\", {})  # ë‚´ë¶€ ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ\n",
    "    type_value = classification_data.get(\"type\", \"ì¼ë°˜ëŒ€í™”\")  # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    # keywords = classification_data.get(\"keywords\", [])  # ê¸°ë³¸ê°’ ì„¤ì •\n",
    "    \n",
    "    print(f\"===================== Type: {type_value}\")\n",
    "    # print(f\"===================== Keywords: {keywords}\")\n",
    "    \n",
    "    if type_value == 'ì •ë³´ê²€ìƒ‰':\n",
    "        return \"ì •ë³´ê²€ìƒ‰ ì²´ì¸ ì‹¤í–‰ì€ ì—¬ê¸°!!!\"\n",
    "    elif type_value == 'ì¶”ì²œìš”ì²­':\n",
    "        formatted_string = format_change(classification_result)\n",
    "        return recommendation_chain.invoke(formatted_string)\n",
    "    else:\n",
    "        return default_chain.invoke({\"classification_result\": classification_result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_chain = (\n",
    "  {\"classification_result\": RunnableLambda(preprocess_classification_result),\n",
    "   \"user_input\":itemgetter(\"user_input\")}\n",
    "  | RunnableLambda(process_user_input)\n",
    "  | StrOutputParser()  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================== Type: ì¶”ì²œìš”ì²­\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'```json\\n[\\n  {\\n    \"id\": \"f09b0b34-df98-48df-a722-7837b5e71f33\",\\n    \"genre\": \"ì•¡ì…˜/ì–´ë“œë²¤ì³\",\\n    \"title\": \"ì•¡ì…˜íˆì–´ë¡œ\"\\n  }\\n]\\n```\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"user_input\": \"ë¯¸êµ­ ë‰´ìš•ì„ ë°°ê²½ìœ¼ë¡œ í•œ ì•¡ì…˜ ì˜í™”ë¥¼ ì¶”ì²œí•´ì¤˜.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
